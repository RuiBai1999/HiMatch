{
  "data": {
    "dataset": "wos",
    "data_dir": "./data",
    "train_file": "wos_train.json",
    "val_file": "wos_val.json",
    "test_file": "wos_test.json",
    "label_desc_file": "wos_label_desc.json",
    "prob_json": "wos_prob.json",
    "prob_child_to_parent_json": "wos_prob_child_parent.json",
    "hierarchy": "wos.taxnomy",
    "total_sample_num": 6,
    "sample_num": 2,
    "positive_num": 1,
    "negative_ratio": 3,
    "layer_num": 2
  },
  "vocabulary": {
    "dir": "vocab_wos",
    "vocab_dict": "word.dict",
    "max_token_vocab": 60000,
    "label_dict": "label.dict"
  },
  "embedding": {
    "token": {
      "dimension": 300,
      "type": "pretrain",
      "pretrained_file": "/dev_data/dev_data/chb/hier-project/datasets/embedding/glove.6B.300d.txt",
      "dropout": 0.5,
      "init_type": "uniform"
    },
    "label": {
      "dimension": 300,
      "type": "random",
      "dropout": 0.5,
      "init_type": "kaiming_uniform"
    }
  },
  "text_encoder": {
    "max_length": 256,
    "RNN": {
      "bidirectional": true,
      "num_layers": 1,
      "type": "GRU",
      "hidden_dimension": 100,
      "dropout": 0.1
    },
    "CNN": {
      "kernel_size": [2, 3, 4],
      "num_kernel": 100
    },
    "topK_max_pooling": 1
  },
  "structure_encoder": {
    "type": "GCN",
    "node": {
      "type": "text",
      "dimension": 300,
      "dropout": 0.05
    }
  },
  "model": {
    "type": "HiMatch",
    "linear_transformation": {
      "text_dimension": 300,
      "node_dimension": 300,
      "dropout": 0.5
    },
    "classifier": {
      "num_layer": 1,
      "dropout": 0.5
    }
  },
  "train": {
    "optimizer": {
      "type": "Adam",
      "learning_rate": 0.0001,
      "lr_decay": 1.0,
      "lr_patience": 5,
      "early_stopping": 50
    },
    "batch_size": 64,
    "start_epoch": 0,
    "end_epoch": 150,
    "loss": {
      "classification": "BCEWithLogitsLoss",
      "recursive_regularization": {
        "flag": false,
        "penalty": 0.000001
      }
    },
    "device_setting": {
      "device": "cuda",
      "visible_device_list": "0",
      "num_workers": 10
    },
    "checkpoint": {
      "dir": "wos_checkpoint",
      "max_number": 10,
      "save_best": ["Macro_F1", "Micro_F1"]
    }
  },
  "eval": {
    "batch_size": 64,
    "threshold": 0.5
  },
  "test": {
    "best_checkpoint": "best_micro_HiAGM-TP",
    "batch_size": 64
  },
  "log": {
    "level": "info",
    "filename": "wos.log"
  }
}